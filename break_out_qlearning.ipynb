{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 00:36:31.173862: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-09 00:36:31.202969: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-09 00:36:31.202997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-09 00:36:31.203807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-09 00:36:31.209533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-09 00:36:31.717692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3444837047, 2669555309)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "max_episodes = 10  # Limit training episodes, will run until solved if smaller than 1\n",
    "\n",
    "# Use the Atari environment\n",
    "# Specify the `render_mode` parameter to show the attempts of the agent in a pop up window.\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\")  # , render_mode=\"human\")\n",
    "# Environment preprocessing\n",
    "env = AtariPreprocessing(env)\n",
    "# Stack four frames\n",
    "env = FrameStack(env, 4)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 00:36:34.991305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.038537: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.038572: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.041221: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.041262: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.041274: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.147260: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.147311: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.147317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-09 00:36:35.147340: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-09 00:36:35.147357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9711 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "num_actions = 4\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    return tf.keras.models.Sequential(\n",
    "        [\n",
    "            layers.Lambda(\n",
    "                lambda tensor: tf.transpose(tensor, [0, 2, 3, 1]),\n",
    "                output_shape=(84, 84, 4),\n",
    "                input_shape=(4, 84, 84),\n",
    "            ),\n",
    "            # Convolutions on the frames on the screen\n",
    "            layers.Conv2D(32, 8, strides=4, activation=\"relu\", input_shape=(4, 84, 84)),\n",
    "            layers.Conv2D(64, 4, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(64, 3, strides=1, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(num_actions, activation=\"linear\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 00:41:39.086356: I external/local_xla/xla/service/service.cc:168] XLA service 0x560240125870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-09 00:41:39.086395: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-05-09 00:41:39.106499: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715211699.235887   18375 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fccc13c6170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fccc13c6170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fccc13c6170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fccc13c6170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Stopped at episode 10!\n"
     ]
    }
   ],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation)\n",
    "    episode_reward = 0\n",
    "    rewards_avg=[]\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "    \n",
    "    if running_reward > 40:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    if (\n",
    "        max_episodes > 0 and episode_count >= max_episodes\n",
    "    ):  # Maximum number of episodes reached\n",
    "        print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc0c043340>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(rewards_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
