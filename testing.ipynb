{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6776193b-3c22-4969-825e-8c81da6b3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, layers\n",
    "import collections\n",
    "import tqdm\n",
    "\n",
    "env = gym.make(\"ALE/IceHockey-v5\", obs_type=\"grayscale\")\n",
    "\n",
    "seed = 21\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c3e09cd8-12d7-4e83-ae86-a90b5cfe6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(Model):\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_actions: int,\n",
    "      num_hidden_units: int):\n",
    "    super().__init__()\n",
    "\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    self.critic = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor):\n",
    "    x = self.common(inputs)\n",
    "    return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "75257e8c-8bf9-4721-9b5f-a4c355fea382",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "78c2c6bd-3c90-47bd-942d-3d4ff4433726",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.numpy_function(Tout=[tf.float32, tf.int32, tf.int32])\n",
    "def env_step(action: np.ndarray):\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state, reward, done, truncated, info = env.step(action)\n",
    "  return (state.astype(np.float32),\n",
    "          np.array(reward, np.int32),\n",
    "          np.array(done, np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "081c30b2-31ef-4e2d-b39f-ec5bd837c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    max_steps: int):\n",
    "    \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "    \n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "    \n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "    \n",
    "    for t in tf.range(max_steps):\n",
    "        # Convert state into a batched tensor (batch size = 1)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        \n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "        \n",
    "        # Sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "        \n",
    "        # Store critic values\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "        \n",
    "        # Store log probability of the action chosen\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "        \n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, reward, done = env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        \n",
    "        # Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "        \n",
    "        if tf.cast(done, tf.bool):\n",
    "          break\n",
    "    \n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    \n",
    "    return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3edfd51-9e78-48bd-9d22-10430e50f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor,\n",
    "    gamma: float,\n",
    "    standardize: bool = True):\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) /\n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1bb60ca7-0708-4a02-802f-b0b17b569939",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,\n",
    "    values: tf.Tensor,\n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "30efa1c5-64c8-41b0-89e3-9343108a2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    optimizer: tf.keras.optimizers.Optimizer,\n",
    "    gamma: float,\n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(\n",
    "        initial_state, model, max_steps_per_episode)\n",
    "\n",
    "    # Calculate the expected returns\n",
    "    returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, returns = [\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
    "\n",
    "    # Calculate the loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50abbfef-b4e7-4920-a7db-9e54700a65c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_15240\\1388323743.py\", line 15, in train_step  *\n        action_probs, values, rewards = run_episode(\n    File \"C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_15240\\2840869305.py\", line 22, in run_episode  *\n        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n\n    ValueError: Shape must be rank 2 but is rank 3 for '{{node while/categorical/Multinomial}} = Multinomial[T=DT_FLOAT, output_dtype=DT_INT64, seed=21, seed2=0](while/actor_critic_8_1/dense_22_1/Add, while/categorical/Multinomial/num_samples)' with input shapes: [1,210,18], [].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:20\u001b[0m\n",
      "File \u001b[1;32mD:\\pythonanaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file333vohvl.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(initial_state, model, optimizer, gamma, max_steps_per_episode)\u001b[0m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 12\u001b[0m     (action_probs, values, rewards) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(run_episode), (ag__\u001b[38;5;241m.\u001b[39mld(initial_state), ag__\u001b[38;5;241m.\u001b[39mld(model), ag__\u001b[38;5;241m.\u001b[39mld(max_steps_per_episode)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m     returns \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(get_expected_return), (ag__\u001b[38;5;241m.\u001b[39mld(rewards), ag__\u001b[38;5;241m.\u001b[39mld(gamma)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m     (action_probs, values, returns) \u001b[38;5;241m=\u001b[39m [ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mld(x), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [ag__\u001b[38;5;241m.\u001b[39mld(action_probs), ag__\u001b[38;5;241m.\u001b[39mld(values), ag__\u001b[38;5;241m.\u001b[39mld(returns)]]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filekti8bfrq.py:68\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_episode\u001b[1;34m(initial_state, model, max_steps)\u001b[0m\n\u001b[0;32m     66\u001b[0m t \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m action \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrange, (ag__\u001b[38;5;241m.\u001b[39mld(max_steps),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), extra_test, loop_body, get_state_1, set_state_1, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_probs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreak_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     69\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(action_probs)\u001b[38;5;241m.\u001b[39mstack, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     70\u001b[0m values \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(values)\u001b[38;5;241m.\u001b[39mstack, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filekti8bfrq.py:32\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_episode.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     30\u001b[0m state \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mld(state), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     31\u001b[0m (action_logits_t, value) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model), (ag__\u001b[38;5;241m.\u001b[39mld(state),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 32\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     33\u001b[0m action_probs_t \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax, (ag__\u001b[38;5;241m.\u001b[39mld(action_logits_t),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     34\u001b[0m values \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(values)\u001b[38;5;241m.\u001b[39mwrite, (ag__\u001b[38;5;241m.\u001b[39mld(t), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msqueeze, (ag__\u001b[38;5;241m.\u001b[39mld(value),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_15240\\1388323743.py\", line 15, in train_step  *\n        action_probs, values, rewards = run_episode(\n    File \"C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_15240\\2840869305.py\", line 22, in run_episode  *\n        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n\n    ValueError: Shape must be rank 2 but is rank 3 for '{{node while/categorical/Multinomial}} = Multinomial[T=DT_FLOAT, output_dtype=DT_INT64, seed=21, seed2=0](while/actor_critic_8_1/dense_22_1/Add, while/categorical/Multinomial/num_samples)' with input shapes: [1,210,18], [].\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "# `CartPole-v1` is considered solved if average reward is >= 475 over 500\n",
    "# consecutive trials\n",
    "reward_threshold = 475\n",
    "running_reward = 0\n",
    "\n",
    "# The discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep the last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "t = tqdm.trange(max_episodes)\n",
    "for i in t:\n",
    "    initial_state, info = env.reset()\n",
    "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "    episode_reward = int(train_step(\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "    # Show the average episode reward every 10 episodesa\n",
    "    if i % 10 == 0:\n",
    "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
    "        break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
